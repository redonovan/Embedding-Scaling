# Embedding-Scaling
<b>Research</b>
  
My <a href=https://github.com/redonovan/Transformer-Encoder>Transformer Encoder</a> implementation turned into a mini research project investigating the use of Embedding scaling in Transformers.  The results may have implications for many more models with Embedded inputs and Softmax/Sigmoid outputs.  Full details are given as comments in transformer_encoder.py.
  
  
